{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecords files\n",
    "\n",
    "* [`tf.slim site`](https://github.com/tensorflow/models/tree/master/research/slim)\n",
    "* 위의 링크에 나온 방법으로 TensorFlow에서 제공하는 `flower` 데이터 셋을 download 받고\n",
    "* 그 후 `TFRecords` format의 데이터로 만들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and converting to TFRecord format\n",
    "\n",
    "```shell\n",
    "$ DATA_DIR=datasets/flowers\n",
    "$ python download_and_convert_data.py \\\n",
    "    --dataset_name=flowers \\\n",
    "    --dataset_dir=\"${DATA_DIR}\"\n",
    "```\n",
    "* script 파일을 그대로 이용하여도 된다.\n",
    "* `flower` 데이터 셋을 download 한 이후 `TFRecords` format으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(r\"C:\\\\flowme\\\\tf-models\\\\models\\\\research\\\\slim\")\n",
    "from datasets import dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'c:\\\\flowme\\\\scripts\\\\python36.zip',\n",
       " 'C:\\\\Users\\\\dddd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\DLLs',\n",
       " 'C:\\\\Users\\\\dddd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\lib',\n",
       " 'C:\\\\Users\\\\dddd\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36',\n",
       " 'c:\\\\flowme',\n",
       " 'c:\\\\flowme\\\\lib\\\\site-packages',\n",
       " 'c:\\\\flowme\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\dddd\\\\.ipython']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL where the Flowers data can be downloaded.\n",
    "_DATA_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "\n",
    "# The number of images in the validation set.\n",
    "_NUM_VALIDATION = 350\n",
    "\n",
    "# Seed for repeatability.\n",
    "_RANDOM_SEED = 0\n",
    "\n",
    "# The number of shards per dataset split.\n",
    "_NUM_SHARDS = 5\n",
    "\n",
    "# The path where the Flowers dataset is\n",
    "dataset_dir = '../datasets/flowers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downlaod `flower` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf.gfile.Exists(dataset_dir):\n",
    "  tf.gfile.MakeDirs(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading flower_photos.tgz 100.0%\n",
      "Successfully downloaded flower_photos.tgz 228813984 bytes.\n"
     ]
    }
   ],
   "source": [
    "# downlaod and uncompress dataset.tar.gz\n",
    "if not tf.gfile.Exists(dataset_dir + '/flower_photos'):\n",
    "  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_filenames_and_classes(dataset_dir):\n",
    "  \"\"\"Returns a list of filenames and inferred class names.\n",
    "\n",
    "  Args:\n",
    "    dataset_dir: A directory containing a set of subdirectories representing\n",
    "      class names. Each subdirectory should contain PNG or JPG encoded images.\n",
    "\n",
    "  Returns:\n",
    "    A list of image file paths, relative to `dataset_dir` and the list of\n",
    "    subdirectories, representing class names.\n",
    "  \"\"\"\n",
    "  flower_root = os.path.join(dataset_dir, 'flower_photos')\n",
    "  directories = []\n",
    "  class_names = []\n",
    "  for filename in os.listdir(flower_root):\n",
    "    path = os.path.join(flower_root, filename)\n",
    "    if os.path.isdir(path):\n",
    "      directories.append(path)\n",
    "      class_names.append(filename)\n",
    "\n",
    "  photo_filenames = []\n",
    "  for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "      path = os.path.join(directory, filename)\n",
    "      photo_filenames.append(path)\n",
    "\n",
    "  return photo_filenames, sorted(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 3670\n",
      "class_names : ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "print('dataset size: {}'.format(len(photo_filenames)))\n",
    "print('class_names : {}'.format(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_to_ids = dict(zip(class_names, range(len(class_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class name: daisy -- index: 0\n",
      "class name: dandelion -- index: 1\n",
      "class name: roses -- index: 2\n",
      "class name: sunflowers -- index: 3\n",
      "class name: tulips -- index: 4\n"
     ]
    }
   ],
   "source": [
    "# print class names to ids\n",
    "for key, value in class_names_to_ids.items():\n",
    "  print('class name: {} -- index: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(_RANDOM_SEED)\n",
    "random.shuffle(photo_filenames)\n",
    "training_filenames = photo_filenames[_NUM_VALIDATION:]\n",
    "validation_filenames = photo_filenames[:_NUM_VALIDATION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: 3320\n",
      "validation dataset size: 350\n"
     ]
    }
   ],
   "source": [
    "print('training dataset size: {}'.format(len(training_filenames)))\n",
    "print('validation dataset size: {}'.format(len(validation_filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageReader(object):\n",
    "  \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    # Initializes function that decodes RGB JPEG data.\n",
    "    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n",
    "    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n",
    "\n",
    "  def read_image_dims(self, sess, image_data):\n",
    "    image = self.decode_jpeg(sess, image_data)\n",
    "    return image.shape[0], image.shape[1] # image_height, image_width\n",
    "\n",
    "  def decode_jpeg(self, sess, image_data):\n",
    "    image = sess.run(self._decode_jpeg,\n",
    "                     feed_dict={self._decode_jpeg_data: image_data})\n",
    "    assert len(image.shape) == 3\n",
    "    assert image.shape[2] == 3\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset_filename(dataset_dir, split_name, shard_id):\n",
    "  output_filename = 'flowers_%s_%05d-of-%05d.tfrecord' % (\n",
    "      split_name, shard_id, _NUM_SHARDS)\n",
    "  return os.path.join(dataset_dir, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  \"\"\"Returns a TF-Feature of int64s.\n",
    "\n",
    "  Args:\n",
    "    values: A scalar or list of values.\n",
    "\n",
    "  Returns:\n",
    "    A TF-Feature.\n",
    "  \"\"\"\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "\n",
    "\n",
    "def bytes_feature(values):\n",
    "  \"\"\"Returns a TF-Feature of bytes.\n",
    "\n",
    "  Args:\n",
    "    values: A string.\n",
    "\n",
    "  Returns:\n",
    "    A TF-Feature.\n",
    "  \"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n",
    "\n",
    "\n",
    "def float_feature(values):\n",
    "  \"\"\"Returns a TF-Feature of floats.\n",
    "\n",
    "  Args:\n",
    "    values: A scalar of list of values.\n",
    "\n",
    "  Returns:\n",
    "    A TF-Feature.\n",
    "  \"\"\"\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n",
    "  \"\"\"Converts the given filenames to a TFRecord dataset.\n",
    "\n",
    "  Args:\n",
    "    split_name: The name of the dataset, either 'train' or 'validation'.\n",
    "    filenames: A list of absolute paths to png or jpg images.\n",
    "    class_names_to_ids: A dictionary from class names (strings) to ids\n",
    "      (integers).\n",
    "    dataset_dir: The directory where the converted datasets are stored.\n",
    "  \"\"\"\n",
    "  assert split_name in ['train', 'validation']\n",
    "\n",
    "  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n",
    "  # num_per_shard = int( 3320 / 5.0 ) = 664 for training dataset\n",
    "  # num_per_shard = int( 350 / 5.0 ) = 70 for validation dataset\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    image_reader = ImageReader()\n",
    "\n",
    "    with tf.Session('') as sess:\n",
    "\n",
    "      for shard_id in range(_NUM_SHARDS):\n",
    "        output_filename = _get_dataset_filename(\n",
    "            dataset_dir, split_name, shard_id)\n",
    "        # output_filename -> '../data/flowers/flowers_train_00000-of-00005.tfrecord'\n",
    "        #                    '../data/flowers/flowers_train_00001-of-00005.tfrecord'\n",
    "        #                                       ...\n",
    "        #                    '../data/flowers/flowers_train_00004-of-00005.tfrecord'\n",
    "        \n",
    "        # step 1\n",
    "        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "          start_ndx = shard_id * num_per_shard\n",
    "          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n",
    "          for i in range(start_ndx, end_ndx):\n",
    "            sys.stdout.write('\\r>> Converting image %d/%d shard %d' % (\n",
    "                i+1, len(filenames), shard_id))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Read the filename:\n",
    "            # step 2\n",
    "            image_data = tf.gfile.FastGFile(filenames[i], 'rb').read()\n",
    "            height, width = image_reader.read_image_dims(sess, image_data)\n",
    "\n",
    "            class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
    "            class_id = class_names_to_ids[class_name]\n",
    "\n",
    "            #example = dataset_utils.image_to_tfexample(\n",
    "            #    image_data, b'jpg', height, width, class_id)\n",
    "            # step 3\n",
    "            features = tf.train.Features(feature={'image/encoded': bytes_feature(image_data),\n",
    "                                                  'image/format': bytes_feature(b'jpg'),\n",
    "                                                  'image/class/label': int64_feature(class_id),\n",
    "                                                  'image/height': int64_feature(height),\n",
    "                                                  'image/width': int64_feature(width),\n",
    "                                                 })\n",
    "            \n",
    "            # step 4\n",
    "            example = tf.train.Example(features=features)\n",
    "            \n",
    "            # step 5\n",
    "            tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "  sys.stdout.write('\\n')\n",
    "  sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TFRecords` format 파일 만드는 방법\n",
    "\n",
    "```python\n",
    "# Step 1: create a writer to write tfrecord to that file\n",
    "tfrecord_writer = tf.python_io.TFRecordWriter(output_filename)\n",
    "\n",
    "# Step 2: get serialized data (binary values and shape of image)\n",
    "# 한 개의 example(우리가 말하는 data 하나)을 만들기 위해 필요한 정보를 모은다.\n",
    "image_data = tf.gfile.FastGFile(filenames[i], 'rb').read()\n",
    "height, width = image_reader.read_image_dims(sess, image_data)\n",
    "class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
    "class_id = class_names_to_ids[class_name]\n",
    "\n",
    "# Step 3: create a tf.train.Features object\n",
    "features = tf.train.Features(feature={'image/encoded': bytes_feature(image_data),\n",
    "                                      'image/format': bytes_feature(image_format),\n",
    "                                      'image/class/label': int64_feature(class_id),\n",
    "                                      'image/height': int64_feature(height),\n",
    "                                      'image/width': int64_feature(width),\n",
    "                                     })\n",
    "\n",
    "# Step 4: create a sample containing of features defined above\n",
    "example = tf.train.Example(features=features)\n",
    "\n",
    "# Step 5: write the sample to the tfrecord file\n",
    "tfrecord_writer.write(sample.SerializeToString())\n",
    "tfrecord_writer.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Converting image 3320/3320 shard 4\n",
      ">> Converting image 350/350 shard 4\n"
     ]
    }
   ],
   "source": [
    "# First, convert the training and validation sets.\n",
    "_convert_dataset('train', training_filenames, class_names_to_ids, dataset_dir)\n",
    "_convert_dataset('validation', validation_filenames, class_names_to_ids, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, write the labels file:\n",
    "labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n",
    "dataset_utils.write_label_file(labels_to_class_names, dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label index: 0 -- class name: daisy\n",
      "label index: 1 -- class name: dandelion\n",
      "label index: 2 -- class name: roses\n",
      "label index: 3 -- class name: sunflowers\n",
      "label index: 4 -- class name: tulips\n"
     ]
    }
   ],
   "source": [
    "# print class names to ids\n",
    "for key, value in labels_to_class_names.items():\n",
    "  print('label index: {} -- class name: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_up_temporary_files(dataset_dir):\n",
    "  \"\"\"Removes temporary files used to create the dataset.\n",
    "\n",
    "  Args:\n",
    "    dataset_dir: The directory where the temporary files are stored.\n",
    "  \"\"\"\n",
    "  filename = _DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dataset_dir, filename)\n",
    "  tf.gfile.Remove(filepath)\n",
    "\n",
    "  # left original jpg files\n",
    "  #tmp_dir = os.path.join(dataset_dir, 'flower_photos')\n",
    "  #tf.gfile.DeleteRecursively(tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished converting the Flowers dataset!\n"
     ]
    }
   ],
   "source": [
    "_clean_up_temporary_files(dataset_dir)\n",
    "print('Finished converting the Flowers dataset!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
